---
title: "PUBLICATIONS"
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: true
    self_contained: false
    code_folding: false
    css:
      - css/GlobalTheme.css
---

```{r dateformating, include=FALSE}
# specify that you want to use english date formatting - added in a separate chunk to not display it in the webpage
Sys.setlocale("LC_TIME", "English")
```

```{r publicationsList, echo=FALSE, results='asis'}
if(!require(scholar)){
    install.packages("scholar")
}
if(!require(stats)){
    install.packages("stats")
}
if(!require(rvest)){
    install.packages("rvest")
}
if(!require(easyPubMed)){
    install.packages("easyPubMed")
}

# load the function from https://www.jhelvy.com/posts/2021-03-25-customizing-distill-with-htmltools-and-css/
# to create button link redirecting to pdf file
source(file.path("R", "Jp-Helveston-Func.R"))

# create some useful functions and data:
####################################
## escape some special chars, german umlauts, ... (adapted from Thomas Hackl' blog post: https://thackl.github.io/automatically-update-publications-with-R-scholar to handle more special character)
char2html <- function(x) {
  dictionary <- data.frame(
    symbol = c("À",	"à",	"Â",	"â",	"Ä",	"ä",	"Æ",	"æ",	"Ç",	"ç",	"È",	"è",	"É",	"é",	"Ê",	"ê",	"Ë",	"ë",	"Î",	"î",	"Ï",	"ï",	"ñ",	"Ö",	"ö",	"Ô",	"ô",	"Œ",	"œ",	"Ù",	"ù",	"Û",	"û",	"Ü",	"ü",	"ß"),
    html = c("&Agrave;",	"&agrave;",	"&Acirc;",	"&acirc;",	"&Auml;",	"&auml;",	"&AElig;",	"&aelig;",	"&Ccedil;",	"&ccedil;",	"&Egrave;",	"&egrave;",	"&Eacute;",	"&eacute;",	"&Ecirc;",	"&ecirc;",	"&Euml;",	"&euml;",	"&Icirc;",	"&icirc;",	"&Iuml;",	"&iuml;",	"&ntilde;",	"&Ouml;",	"&ouml;",	"&Ocirc;",	"&ocirc;",	"&OElig;",	"&oelig;",	"&Ugrave;",	"&ugrave;",	"&Ucirc;",	"&ucirc;",	"&Uuml;",	"&uuml;",	"&szlig;")
  )
  for (i in 1:dim(dictionary)[1]) {
    x <- gsub(dictionary$symbol[i], dictionary$html[i], x)
  }
  x
}

## remove some elements form the publication' list
toRemove <- function(toExclude, from, data) {
  Res <- c()
  for (i in seq_along(toExclude)) {
    if(length(grep(toExclude[[i]], data[[from]])) == 0){
      next
    }else{
    Res <- c(Res, grep(toExclude[[i]], data[[from]]))
  }
  }
  if (length(which(duplicated(Res))) > 0) {
    Res <- Res[-which(duplicated(Res)),]
  }
  if(is.null(Res)){
    cleaned <- data
  }else{
  cleaned <- data[-Res, ]
  }
  return(cleaned)
}

## Look for a given element within lists or dataframes
listGet <- function(myList, toFind) {
  #### if myList is a simple list or a df
  if (toFind %in% names(myList) == TRUE) {
    return(myList[[toFind]])
  }
  #### if myList is a nested list
  else if (any(sapply(myList, class) == "list")) {
    for (i in myList) {
      found <- Recall(i, toFind)
      if (!is.null(found)) {
        return(found)
      } else {
        NULL
      }
    }
  }
}

## specify the HTML node (class) to look for within the publisher website (useful if you want display the DOI)
DOIHTMLClass <-
  data.frame(
    Publisher = c(
      "Elsevier",
      "Pergamon",
      "Springer",
      "Frontiers",
      "TheRoyalSocietyPublishing",
      "WileyOnlineLibrary",
      "ColdSpringHarborLaboratory"
    ),
    HTMLNode = c(
      "NotAllowed",
      ".ArticleIdentifierLinks .doi",
      ".c-bibliographic-information__value",
      ".header-bar-three",
      ".epub-section",
      "NotAllowed", 
      ".highwire-cite-metadata-doi"
      )
  )

####################################

# Let's start to construct the publication' list
######################################################

# my google scholar profile url
GscholarURL <-
  "https://scholar.google.com/citations?user=GMudi1sAAAAJ&hl=en"

# retrieve my id from my google scholar profile url (the id is located between "=" and "&")
Qpetitjean <- regmatches(GscholarURL,
                         regexpr("(?<=\\=)(.*?)(?=\\&)",
                                 GscholarURL, perl = T))

# pull the informations from google scholar
html1 <- scholar::get_publications(Qpetitjean)

# remove the oral communications and posters from the publication list (see function above)
html1 <- toRemove(
  toExclude = c("Oral Communication", "oral communication", "Poster", "poster", "Presses universitaires de Franche-Comté"),
  from = "journal",
  data = html1
)

# sometimes journal' names and authors' list can be truncated. 
# also volume and pages of the publications as well as the publication URL and the DOI are not included in the dataset
# We will hence retrieve the full list of authors names, the full journal name as well as the volume and pages and the DOI for each publication

## specify the list of info to retrieve
toRetrieve <- c("Authors", "Journal", "Volume", "Pages", "PubUrl", "DOI")

## allocate space within an empty dataframe
Res <-
  stats::setNames(data.frame(matrix(
    ncol = length(toRetrieve), nrow = nrow(html1)
  )), toRetrieve)

## loop over the publications to retrieve the complementary informations
for (i in seq(nrow(html1))) {
  PubMedDataXML <- NULL
  ### get the infos
  temp <-
    scholar::get_publication_data_extended(Qpetitjean, html1[i, "pubid"])
  
  ### if "PubUrl" is specified in toRetrieve, look for the publication URL
  if ("PubUrl" %in% toRetrieve) {
    PubUrl <-
      scholar::get_publication_url(Qpetitjean, html1[i, "pubid"])
    temp <- cbind(temp, PubUrl)
  }
  
  #### use PubMed data
  PubMedData <-
    easyPubMed::get_pubmed_ids_by_fulltitle(html1[i, "title"])
 
  tryCatch({
    PubMedDataXML <- R.utils::withTimeout({
      easyPubMed::fetch_pubmed_data(PubMedData)
    }, timeout = 5)
  }, error = function(err)
    NULL)
  
  if (!is.null(PubMedDataXML)) {
    DOI <- regmatches(
      PubMedDataXML,
      regexpr("10.\\d{4,9}/[-._;()/:a-z0-9A-Z]+", PubMedDataXML, perl = F)
    )
    #### or if it does not work, retrieve the DOI from from the URL of the article (e.g., Wiley)
  } else if (!is.null(temp[["Publisher"]])){
    if (DOIHTMLClass[which(DOIHTMLClass[["Publisher"]] == gsub(" .*$", "", gsub(" ", "", temp[["Publisher"]]))),
                          "HTMLNode"] == "NotAllowed") {
    DOI <-
      regmatches(temp[["PubUrl"]],
                 regexpr("10.\\d{4,9}/[-._;()/:a-z0-9A-Z]+", temp[["PubUrl"]], perl = F))
    #### otherwise, try to retrieve the DOI from the publisher website (webscrapping) according to the DOIHTMLClass
  } else{
    DOI <-
      rvest::html_text(rvest::html_nodes(rvest::read_html(temp[["PubUrl"]]),
                                         DOIHTMLClass[which(DOIHTMLClass[["Publisher"]] == gsub(" .*$", "", gsub(" ", "", temp[["Publisher"]]))),
                                                      "HTMLNode"]))

    ##### extract the DOI from other text elements
    DOI <- data.frame(DOI = regmatches(
      DOI,
      regexpr("10.\\d{4,9}/[-._;()/:a-z0-9A-Z]+", DOI, perl = F)
    ))
    if(nrow(DOI) > 1){
      DOI <- unique(DOI)
    }
  }
  } else {
    DOI <- ""
  }
  temp <-
    cbind(temp, DOI)
  ### fill the Res dataframe with the retrieved informations
  Res[i, ] <-
    stats::setNames(as.data.frame(t(sapply(toRetrieve, function(x)
      listGet(temp, x)))), toRetrieve)
}

# bind the Res dataframe to html1
html1 <- cbind(html1, Res)

# To use the Authors full list, we need keep only the first letter of the firstname and the whole last name
AuthorsFUll <- apply(html1, 1, function(y) {
  temp <- unlist(strsplit(y[["Authors"]], ", "))
  # manage hyphenation in firstname
  Fn <- sub("^(.*?)\\s.*$", "\\1", temp)
  Ln <- sub("^[^ ]* ", "", temp)
  Ln <- gsub("(^|-)(.)", "\\1\\U\\2", tolower(Ln), perl = TRUE)
  hyphenPos <- which(grepl("-", Fn) == TRUE)
  FnIni <- substr(Fn, 1, 1)
  if (length(hyphenPos) > 0) {
    toShorten <- strsplit(Fn[hyphenPos], "-")
    Shortened <- lapply(toShorten, function(z) {
      paste(substr(z, 1, 1), collapse = ".")
    })
    Shortened <- gsub("\\.", ".-", Shortened)
    FnIni[hyphenPos] <- Shortened
  }
  FnIni <- paste0(FnIni, ".,")
  # merge firstName first letter and lastname and add dots and comma according to APA style
  temp <- paste(paste0(Ln, ","), FnIni, collapse = " ")
  ## remove the last comma
  temp <- substr(temp, 1, nchar(temp) - 1)
  return(temp)
})

html1 <- cbind(html1, AuthorsFUll)

# add tag specifying bold text (HTML language) to highltght my own name
html1[["AuthorsFUll"]] <-
  gsub("PETITJEAN, Q.", "<strong>PETITJEAN, Q.</strong>", html1[["AuthorsFUll"]])

# split the publication record according to the year of publication
html2 <- split(html1, html1[["year"]])

# order the list' elements according to year in a descending way
html2 <- html2[sort(names(html2), decreasing = T)]

# reorder the various elements (authors, year, title, journal...) from the publication record
# also add the publication' URL behind the publication' title (publisher website), the PDF file and the altimetric and dimension badges below each publication
# see https://api.altmetric.com/embeds.html and https://badge.dimensions.ai/#build for more details

# load the file containing the correspondence between the publication title and the pdf file name located in the github repository or a weblink to the pdf when #the article is OA
pubList <- read.csv(list.files("PubList",
                                pattern = ".csv",
                                full.names = TRUE),
                    sep = ";",
                    dec = ".",
                    encoding = "latin1")

# NA may interfer with the next code bloc, replace them by none
pubList[is.na(pubList)] <- ""

html3 <- list()
for (i in names(html2)) {
  temp <- list()
  for(j in seq(nrow(html2[[i]]))){
    dat <- html2[[i]][j,]
    # create the line containing authors names, publication title with its URL
    AuthTitle <- paste0(
      "<table style='border: none;'><td width='100%'>",
      # add a bullet in front of each publication
      fontawesome::fa("square-caret-right", fill = "#333333"),
      # create link to publisher website for each publication
      paste(
        dat[["AuthorsFUll"]],
        paste0("(", dat[["year"]], ")."),
        paste0("<a href=",
               dat[["PubUrl"]],
               ">",
               dat[["title"]],
               "</a>"),
        sep = " "
      ),
      "</a>"
    )
    # concatenate the journal name, volume, page and DOI (when present) to the line containing authors names, publication title with its URL
    AuthTitlePub <- paste(paste(paste(AuthTitle,
                                      dat[["journal"]], sep = ". "),
                                dat[["Volume"]],
                                dat[["Pages"]],
                                sep = ", "),
                          ifelse(dat[["DOI"]] == '', dat[["DOI"]], paste("doi:", dat[["DOI"]], sep = " ")),
                          sep = ". ")
    
    # add a dot and break the line at the end of the publication reference
    AuthTitlePubEnd <- paste(AuthTitlePub,
                             "<br/>",
                             sep = ".")
    
    
    # make the correspondence between publication title and the link to download the pdf (either a pdf stored in github repository or a web link to     the pdf when the article is OA)
    ## retrieve extension of the publication file in the pubList
    datHtml <- char2html(dat[["title"]])
    pubListTitleHTML <- char2html(pubList[["Title"]])
    ext <- pubList[which(tolower(gsub(
                "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
              )) == tolower(gsub(
                "[^[:alnum:]]", " ", datHtml, perl = T
              ))), "Extension"]
    
    # create the lines to generate the icon
    pdfLink <- icon_link(
            icon = "fa fa-file-pdf",
            text = " Full-text",
            url =  ifelse(ext == ".pdf",
              paste0("PubList/", pubList[which(tolower(gsub(
                "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
              )) == tolower(gsub(
                "[^[:alnum:]]",              " ", datHtml, perl = T
              ))), "FileName"], pubList[which(tolower(gsub(
                "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
              )) == tolower(gsub(
                "[^[:alnum:]]", " ", datHtml, perl = T
              ))), "Extension"]),
              pubList[which(tolower(gsub(
                "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
              )) == tolower(gsub(
                "[^[:alnum:]]", " ", datHtml, perl = T
              ))), "FileName"]
            )
          )
    
    # add the pdf icon and the link to pdf to the publication reference
       AuthTitlePubEndLink <- paste(
          AuthTitlePubEnd, 
          pdfLink,
          sep = " ")
       
    # in case the data are open access, add the DATA icon and the link to data repository
          if (pubList[which(tolower(gsub(
            "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
          )) == tolower(gsub(
            "[^[:alnum:]]", " ", datHtml, perl = T
          ))), "Data"] != "") {
            AuthTitlePubEndLink <- paste(AuthTitlePubEndLink,
            icon_link(icon = "fa fa-database",
                      text = " DATA",
                      url =
                        pubList[which(tolower(gsub(
                          "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
                        )) == tolower(gsub(
                          "[^[:alnum:]]", " ", datHtml, perl = T
                        ))), "Data"]),
            
          sep = " ")
          }
       
       # in case the code is open access, add the code icon and the link to code repository
          if (pubList[which(tolower(gsub(
            "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
          )) == tolower(gsub(
            "[^[:alnum:]]", " ", datHtml, perl = T
          ))), "Code"] != "") {
            AuthTitlePubEndLink <- paste(AuthTitlePubEndLink,
            icon_link(icon = "fa fa-file-code-o",
                      text = " R CODE",
                      url =
                        pubList[which(tolower(gsub(
                          "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
                        )) == tolower(gsub(
                          "[^[:alnum:]]", " ", datHtml, perl = T
                        ))), "Code"]),
          sep = " ")
          }
       
          # in case their is an online statistical report (Rmarkdown), add the code icon and the link to the report
          if (pubList[which(tolower(gsub(
            "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
          )) == tolower(gsub(
            "[^[:alnum:]]", " ", datHtml, perl = T
          ))), "StatReport"] != "") {
            AuthTitlePubEndLink <- paste(AuthTitlePubEndLink,
            icon_link(icon = "fas fa-file-invoice",
                      text = " RMarkdown",
                      url =
                        pubList[which(tolower(gsub(
                          "[^[:alnum:]]", " ", pubListTitleHTML, perl = T
                        )) == tolower(gsub(
                          "[^[:alnum:]]", " ", datHtml, perl = T
                        ))), "StatReport"]),
          sep = " ")
          }
       
    # add the altimetric and dimension badges to the publication reference
      AuthTitlePubEndLinkAltim <- paste0(AuthTitlePubEndLink,
        ifelse(
          dat[["DOI"]] == '',
          "",
          paste0(
            "<div style='display: flex; space-between; width: 140px;'>",
            "<div data-badge-popover='right' data-badge-type='1' data-doi=",
            dQuote(dat[["DOI"]], q = F),
            "class='altmetric-embed'></div>",
            " &ensp; ",
            "<span class='__dimensions_badge_embed__' data-doi=",
            dQuote(dat[["DOI"]], q = F),
            "data-legend='hover-right' data-style='large_rectangle'></span><script async src='https://badge.dimensions.ai/badge.js' charset='utf-8'></script>",
            "</div>"
          )
        )
      )
       
    # break the line after publication reference
    AuthTitlePubEndLinkAltim <- paste(AuthTitlePubEndLinkAltim,
      "</table>",
      sep = "&ensp;")
    
    temp[[j]] <- AuthTitlePubEndLinkAltim
  }
  
  # remove special characters (see function above)
  temp <- as.list(char2html(temp))
  
  # add a header for each years section
  temp <- c(paste0(" \n## ", i, " \n "),
            temp)
  html3 <- unlist(c(html3, temp))
}

# in case there is some null values (e.g., journal, volume, pages...), remove the extra comma
html3 <- gsub("NULL", "", html3)
html3 <- gsub("(, )+", ", ", html3)

# also, if there is null values at the end of the string, the comma might be followed a period, in this case remove the comma
html3 <- gsub('(, \\.)|(,\\.)', ".", html3)

# add a sentence to the upper right corner of the page to specify the date at which the publication list has been updated
# (from Thomas Hackl' blog post: https://thackl.github.io/automatically-update-publications-with-R-scholar)
html4 <- c(
  paste0(
    "<div class='small-text'>Last updated ",
    format(Sys.Date(), format = "%B %d, %Y"),
    "&nbsp;&ndash;&nbsp;Pulled automatically from <a href='https://scholar.google.com/citations?user=GMudi1sAAAAJ&hl=en'>Google Scholar</a>.</div>",
    "<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>"
  ),
  html3
)
# display the output
cat(html4)
```